{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d7ef05-5c8e-4201-9d86-343d2bd92548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9ba801-48af-4fa8-a43d-779db5b841db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want our coefficent high and our loss low!\n",
    "# in general ML: penalty on high loss, so only high coefficients should get through and get higher accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e01fa0e-aa2c-40a8-bd95-39a0b6d5a386",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 0.00001 # add a super small number to ensure we won't run into a division by 0 (aka the contribution it'll make is virtually nonexistent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b1b733-1428-4082-bed0-9fa16a918822",
   "metadata": {},
   "source": [
    "Dice Loss test (Dice coefficient as evaluation metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c9851-59a6-4103-b532-29d735d24496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def Dice_loss(y_true, y_pred):\n",
    "    return 1.-dice_coef(y_true, y_pred)\n",
    "\n",
    "# call the function\n",
    "model.compile(optimizer=Adam(lr=1e-2), loss=Dice_loss, metrics=[Dice_coef])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7463d627-a747-4bd5-a7e5-b56f57f910ec",
   "metadata": {},
   "source": [
    "BCE Dice Loss test (Dice coefficient as evaluation metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f99d9-e7aa-41af-9b94-eb18b4fac098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "# coefficient for here is the Dice coefficient too\n",
    "\n",
    "def BCE_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + Dice_loss(y_true, y_pred)\n",
    "\n",
    "# call the function\n",
    "model.compile(optimizer = Adam(lr = 1e-2), loss=BCE_loss, metrics=[Dice_coef])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399b1d14-93b6-4e24-b9c6-bebb7aa150c0",
   "metadata": {},
   "source": [
    "Jaccard Loss test (Jaccard coefficient as evaluation metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decaab87-7b8b-442e-a98d-c3c2f9b3c052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard_coef(y_true, y_predicted): # true and predicted binary mask (function that maps bit image into 0 and 1 pixel values)\n",
    "    intersection = k.sum(y_true*y_predicted, axis=[2, -1, -2]) # k.sum for element wise multiplication, over what axis will the intersection be mapped\n",
    "    sum_union = k.sum(y_true+y_predicted, axis=[2, -1, -2]) # union so addition\n",
    "    JC = (intersection+smooth)/(sum_union-intersection+smooth) # formula for Jaccard coeff, but added smooth to prevent division by a zero!\n",
    "    return k.mean(JC) # return mean of Jaccard coeffs (mean for the whole image dataset, 50 pics? 50 JC coeffs -> take mean to get 1)\n",
    "\n",
    "def Jaccard_loss(y_true, y_predicted):\n",
    "    return 1-Jaccard_coef(y_true, y_predicted) # make sure not JC!! bc that's inside the other func\n",
    "\n",
    "# call the function\n",
    "model.compile(optimizer=Adam(lr=1e-2), loss=Jaccard_loss, metrics=[Jaccard_coef]) # changed from 4 to 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b6b75-e2dc-4512-a95e-c452fa17aee9",
   "metadata": {},
   "source": [
    "Focal Loss test (Dice coefficient as evaluation metric)\n",
    "- address imbalances when doing image detection\n",
    "- applies modulating term\n",
    "- use gamma distribution to dynamically scale entropy loss (scaling factor is gamma)\n",
    "- we can use dice coefficient\n",
    "- focal loss is a generaliation of BCE/binary cross entropy which is used in dice loss, where you account for misclassified errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d705b44-5e52-4d26-ae09-d5e727a54cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focal Loss  (pt) = -αt(1-  pt)^γ log(pt) #here's the equation\n",
    "\n",
    "# uses the Dice coefficient instead of its own as well\n",
    "\n",
    "def Focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred)) # pt is probability\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "# call the function\n",
    "model.compile(optimizer=Adam(lr=1e-2), loss=Focal_loss, metrics=[Dice_coef]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SPR24ENV)",
   "language": "python",
   "name": "spr24env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
