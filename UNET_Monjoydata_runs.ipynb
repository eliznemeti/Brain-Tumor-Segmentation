{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a53daf-6cb6-4c54-a8b0-9d0fb1fd0066",
   "metadata": {},
   "source": [
    "Pip Install all Packages via Terminal in the Correct activated VENV\n",
    "OR !pip install package below\n",
    "\n",
    "For local installs: source UNETENV/bin/activate \n",
    "For cluster installs: source UNETvenv/bin/activate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a8f0e5-195e-4531-a640-9107c484b0fd",
   "metadata": {},
   "source": [
    "Install TensorFlow version that includes support for Appleâ€™s Metal API (need M1/M2 chip). This will allow TensorFlow to utilize the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f11281f8-3f8e-40e5-9dc8-8fe061e65abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-macos\n",
    "!pip install tensorflow-metal\n",
    "\n",
    "import tensorflow as tf # (Ensure using 2.13.0 as mentioned??)\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available; TensorFlow is using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf444ca3-3c43-423e-a6da-f92e59a8daa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import datetime\n",
    "import cv2  # opencv+python == 4.9.0.80 # image processing\n",
    "import matplotlib.pyplot as plt\n",
    "import hdf5storage  # 0.1.19\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tf.keras specifically tailored to work with TensorFlow's backend seamlessly -> better compatibility/performance vs standalone keras\n",
    "from tensorflow.keras import backend as K # make sure all K are capital\n",
    "from tensorflow.keras.models import Model  # Import specific classes\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.optimizers import Adam  # import specific optimizers\n",
    "from tensorflow.keras.optimizers.legacy import Adam # trying legacy Adam for better performance\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "\n",
    "# Monjoy data specific packages\n",
    "import nibabel as nib # to read nifti files (can read compressed .nii.gz files!)\n",
    "from glob import glob # helps pull out files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d6d6b8-0819-43f9-8e45-2147b589cdcb",
   "metadata": {},
   "source": [
    "About the Data\n",
    "\n",
    "- fast scan gastrointestinal motility data with high temporal resolution\n",
    "- each file (e.g. T1w_1.nii.gz) represents the Nth GI motility data in the experiment\n",
    "- 2D spatial information (128x128)\n",
    "- 4 different OBLIQUE slices at differnt DEPTHS per FILE\n",
    "- 150 frames per SLICE\n",
    "- 4d array (128 height x 128 width x 4 slices x 150 frames)\n",
    "- dont batch trials while processing so we can compare VNS effects across trials later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4689b994-7aa8-4076-9281-72c53fb90073",
   "metadata": {},
   "source": [
    "Data Loading/Preprocessing/Image Normalizing\n",
    "\n",
    "- read each .nii.gz file in the specified directory\n",
    "- can work with .nii.gz files directly, without a separate step to unzip them\n",
    "- keeping them zipped can help with storage throughout process\n",
    "- convert it to a NumPy array\n",
    "- normalizes the pixel values to the range [0, 255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d94881b-b2fd-4ea9-b702-ddbdac195bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in /Users/elizabethnemeti/Desktop/data:\n",
      "T1w_1.nii.gz\n",
      "T1w_3.nii.gz\n",
      "T1w_7.nii.gz\n",
      "T1w_12.nii.gz\n",
      "T1w_10.nii.gz\n",
      "T1w_5.nii.gz\n",
      "T1w_9.nii.gz\n",
      "T1w_2.nii.gz\n",
      "T1w_13.nii.gz\n",
      "T1w_6.nii.gz\n",
      "T1w_4.nii.gz\n",
      "T1w_8.nii.gz\n",
      "T1w_11.nii.gz\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/elizabethnemeti/Desktop/' # local runs\n",
    "data_dir = os.path.join(path, 'data') # local runs\n",
    "# data_dir = '/mnt/beegfs/labs/mahmoudilab/nemeti/UNET_tests/data' # cluster runs\n",
    "\n",
    "if not os.path.isdir(data_dir):\n",
    "    raise FileNotFoundError(f\"No such directory: {data_dir}\") # check if data's there\n",
    "\n",
    "files = glob(os.path.join(data_dir, '*.nii.gz')) # pull out all .nii.gz files\n",
    "file_names = [os.path.basename(file) for file in files]\n",
    "print(\"Files in\", data_dir + \":\") # list these .nii.gz files (with just the file name)\n",
    "print(*file_names, sep=\"\\n\") # as a list\n",
    "\n",
    "# iterate and process these .nii.gz files\n",
    "for file_path in files:\n",
    "    img_data = nib.load(file_path) # nibabel takes care of decompression when loading the file\n",
    "    volume = img_data.get_fdata()\n",
    "\n",
    "    # Normalize and convert the image to uint8 if needed\n",
    "    img = np.clip(volume, 0, np.max(volume))  # Clip the image to handle potential outliers\n",
    "    img = (img / np.max(img) * 255).astype('uint8')  # Normalize to 0-255 and convert to uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75421939-d266-4fa4-9bcf-809bbdffb2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process file /Users/elizabethnemeti/Desktop/data/T1w_1.nii.gz: cannot reshape array of size 39321600 into shape (512,512)\n",
      "Failed to process file /Users/elizabethnemeti/Desktop/data/T1w_3.nii.gz: cannot reshape array of size 39321600 into shape (512,512)\n",
      "Failed to process file /Users/elizabethnemeti/Desktop/data/T1w_7.nii.gz: cannot reshape array of size 39321600 into shape (512,512)\n",
      "Failed to process file /Users/elizabethnemeti/Desktop/data/T1w_12.nii.gz: cannot reshape array of size 39321600 into shape (512,512)\n",
      "Failed to process file /Users/elizabethnemeti/Desktop/data/T1w_10.nii.gz: cannot reshape array of size 39321600 into shape (512,512)\n",
      "Failed to process file /Users/elizabethnemeti/Desktop/data/T1w_5.nii.gz: cannot reshape array of size 39321600 into shape (512,512)\n",
      "Failed to process file /Users/elizabethnemeti/Desktop/data/T1w_9.nii.gz: cannot reshape array of size 39321600 into shape (512,512)\n",
      "Failed to process file /Users/elizabethnemeti/Desktop/data/T1w_2.nii.gz: cannot reshape array of size 39321600 into shape (512,512)\n",
      "Failed to process file /Users/elizabethnemeti/Desktop/data/T1w_13.nii.gz: cannot reshape array of size 39321600 into shape (512,512)\n",
      "Failed to process file /Users/elizabethnemeti/Desktop/data/T1w_6.nii.gz: cannot reshape array of size 39321600 into shape (512,512)\n",
      "Failed to process file /Users/elizabethnemeti/Desktop/data/T1w_4.nii.gz: cannot reshape array of size 39321600 into shape (512,512)\n",
      "Failed to process file /Users/elizabethnemeti/Desktop/data/T1w_8.nii.gz: cannot reshape array of size 39321600 into shape (512,512)\n",
      "Failed to process file /Users/elizabethnemeti/Desktop/data/T1w_11.nii.gz: cannot reshape array of size 39321600 into shape (512,512)\n",
      "\n",
      "Finished loading and processing data.\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_data(data_dir, image_dimension=128):\n",
    "    images = []\n",
    "    masks = []\n",
    "    labels = []\n",
    "    files = os.listdir(data_dir)\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    for i, file in enumerate(files, start=1):\n",
    "        try:\n",
    "            # Load the .mat file\n",
    "            mat_file = hdf5storage.loadmat(os.path.join(data_dir, file))['cjdata'][0]\n",
    "            \n",
    "            # Resize image and mask\n",
    "            image = cv2.resize(mat_file[2], dsize=(image_dimension, image_dimension), interpolation=cv2.INTER_CUBIC)\n",
    "            mask = cv2.resize(mat_file[4].astype('uint8'), dsize=(image_dimension, image_dimension), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "            # Normalize the resized image using MinMaxScaler\n",
    "            image = image.astype(np.float32).reshape(-1, 1)  # Flatten the image to fit the scaler input\n",
    "            image = scaler.fit_transform(image).reshape(image_dimension, image_dimension)  # Reshape back after scaling\n",
    "\n",
    "            # Append processed image and mask to their respective lists\n",
    "            images.append(image) # appending the resized images to the variable of the initial images\n",
    "            masks.append(mask.astype(bool)) # aka booleans only, is there a mask or not? \n",
    "            labels.append(int(mat_file[0]))  # fixing our labels because of the transpose \n",
    "\n",
    "            if i % 10 == 0:\n",
    "                sys.stdout.write(f'\\r[{i}/{len(files)}] images loaded: {i / float(len(files)) * 100:.1f} %')\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process file {file}: {e}\")\n",
    "            \n",
    "    print(\"\\nFinished loading and processing data.\")\n",
    "    \n",
    "    return np.array(images), np.array(masks), np.array(labels) #converting labels to numpy arrays\n",
    "\n",
    "data_dir = '/Users/elizabethnemeti/Desktop/data' # local runs\n",
    "# data_dir = '/mnt/beegfs/labs/mahmoudilab/nemeti/UNET_tests/data' # cluster runs\n",
    "# add in cluster path here\n",
    "image_dimension = 512\n",
    "images, masks, labels = load_and_preprocess_data(data_dir, image_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a8fe6d-49fc-4317-843a-821c3995c088",
   "metadata": {},
   "source": [
    "Histogram Equalizer (method 1)\n",
    "- improves the contrast in an image by stretching out the intensity range of the histogram\n",
    "- cv2.equalizeHist function expects a single-channel image (grayscale) as input, and it must be of type uint8\n",
    "- each pixel value gets x by 255\n",
    "- why uint8? to cast as a type that is unassigned (no +/-), (common when working with image intensity)\n",
    "- need the .0 bc equalizer spits out float, so to help it in 255.0)\n",
    "\n",
    "Original: \n",
    "# image = cv2.equalizeHist((image * 255).astype(np.uint8))/255.0\n",
    "\n",
    "Contrast Adjustment (alternative method)\n",
    "- select one at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18c8f51c-6adf-4f19-9aca-751f14699f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/3064 images\n",
      "Processed 200/3064 images\n",
      "Processed 300/3064 images\n",
      "Processed 400/3064 images\n",
      "Processed 500/3064 images\n",
      "Processed 600/3064 images\n",
      "Processed 700/3064 images\n",
      "Processed 800/3064 images\n",
      "Processed 900/3064 images\n",
      "Processed 1000/3064 images\n",
      "Processed 1100/3064 images\n",
      "Processed 1200/3064 images\n",
      "Processed 1300/3064 images\n",
      "Processed 1400/3064 images\n",
      "Processed 1500/3064 images\n",
      "Processed 1600/3064 images\n",
      "Processed 1700/3064 images\n",
      "Processed 1800/3064 images\n",
      "Processed 1900/3064 images\n",
      "Processed 2000/3064 images\n",
      "Processed 2100/3064 images\n",
      "Processed 2200/3064 images\n",
      "Processed 2300/3064 images\n",
      "Processed 2400/3064 images\n",
      "Processed 2500/3064 images\n",
      "Processed 2600/3064 images\n",
      "Processed 2700/3064 images\n",
      "Processed 2800/3064 images\n",
      "Processed 2900/3064 images\n",
      "Processed 3000/3064 images\n",
      "Processed 3064/3064 images\n"
     ]
    }
   ],
   "source": [
    "# Histogram Equalizer\n",
    "\n",
    "equalized_images = [] # store the equalized images\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    uint8_image = (image * 255).astype(np.uint8)  # scale to 255 and convert to uint8\n",
    "    equalized_image = cv2.equalizeHist(uint8_image) # enhance the image contrast\n",
    "    equalized_image = equalized_image.astype(np.float32) / 255.0 # convert equalized image back to regular scale 0-1\n",
    "    equalized_images.append(equalized_image) # add equalized images to empty list\n",
    "\n",
    "    if (i + 1) % 100 == 0 or i == len(images) - 1: # progress bar\n",
    "        print(f'Processed {i + 1}/{len(images)} images')\n",
    "\n",
    "images = np.array(equalized_images) # convert to numpy array for later processing\n",
    "\n",
    "# Contrast Adjustment (alternative method)\n",
    "# alpha = 1.5\n",
    "# beta = 10\n",
    "# image = cv2.convertScaleAbs(image, alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770fd1a1-cecb-46bd-84eb-dfa66894684f",
   "metadata": {},
   "source": [
    "Contrast Adjustment (OR method 2)\n",
    "- src = image\n",
    "- alpha = contrast (remember alpha is multiplied by source) \n",
    "- beta = brightness (remember beta is added to source)\n",
    "- convert = saturate the image\n",
    "- increase brightness b > 0, decrease b < 0\n",
    "- increase contrast a > 1 (cant multiply by 0!), or decrease a < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5d37d45-ac34-466b-8d3b-7e0d04d3d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving locally (optional, but useful)\n",
    "save_path = '/Users/elizabethnemeti/Desktop/UNET_preprocessed_files' # local runs\n",
    "# save_path = '/mnt/beegfs/labs/mahmoudilab/nemeti/UNET_tests/UNET_preprocessed_files' # cluster runs\n",
    "np.save(os.path.join(save_path, 'labels.npy'), labels)\n",
    "np.save(os.path.join(save_path, 'images.npy'), images)\n",
    "np.save(os.path.join(save_path, 'masks.npy'), masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdfdded-a7bc-409a-a14d-04b23d5e0552",
   "metadata": {},
   "source": [
    "Splitting the dataset into Training, Validation and Test Set\n",
    "\n",
    "- Split Training and Testing Data -> 80:20 split\n",
    "- Split Test Set for Validation -> \n",
    "    - split test set into 2 EQUAL parts to get cross-validation set and final test set\n",
    "    - each represents 10% of original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "49b7e6e7-2879-4ee2-9283-468fdb4470a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 2451\n",
      "number of test examples = 306\n",
      "number of validation examples = 307\n",
      "Images_train shape: (2451, 512, 512)\n",
      "Masks_train shape: (2451, 512, 512)\n",
      "Images_test shape: (306, 512, 512)\n",
      "Masks_test shape: (306, 512, 512)\n",
      "Images_val shape: (307, 512, 512)\n",
      "Masks_val shape: (307, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "# Testing and Training Split -> 80:20\n",
    "images_train, images_test, masks_train, masks_test = train_test_split(images,masks,test_size=0.2,train_size=0.8,random_state=1)\n",
    "\n",
    "# Cross Validation Split -> 2 equal parts each 10%\n",
    "images_test, images_val, masks_test, masks_val = train_test_split(images_test,masks_test,test_size = 0.5,train_size =0.5,random_state=1)\n",
    "\n",
    "print (\"number of training examples = \" + str(images_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(images_test.shape[0]))\n",
    "print (\"number of validation examples = \" + str(images_val.shape[0]))\n",
    "print (\"Images_train shape: \" + str(images_train.shape))\n",
    "print (\"Masks_train shape: \" + str(masks_train.shape))\n",
    "print (\"Images_test shape: \" + str(images_test.shape))\n",
    "print (\"Masks_test shape: \" + str(masks_test.shape))\n",
    "print (\"Images_val shape: \" + str(images_val.shape))\n",
    "print (\"Masks_val shape: \" + str(masks_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364627f1-0eff-4b6d-88ac-b0ee75f8db9b",
   "metadata": {},
   "source": [
    "Define Coefficients and Loss Functions\n",
    "\n",
    "Dice coef -> calculates the overlap between the predicted and true masks in segmentation\n",
    "Dice loss -> minimize difference between the predicted and true masks\n",
    "BCE (+ Dice loss) -> measures distance between actual and predicted probabilities; leverages both pixel-wise losses and region-based losses, promoting better segmentation results\n",
    "Jaccard coeff -> measures similarity/diversity between predicted and true masks (0-1 val again)\n",
    "Jaccard loss -> minimize loss\n",
    "Focal -> loss function that addresses class imbalance by giving more weight to hard-to-classify examples\n",
    "\n",
    "- For loss functions like BCE loss, Jaccard loss, and Focal loss, you want to monitor the loss itself ('BCE_loss', 'Jaccard_loss', or 'Focal_loss') because you want to minimize it\n",
    "- For coefficients -> you're looking to maximize you would monitor 'Dice_coef' or 'Jaccard_coef'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "226657dc-96ed-4d12-9219-e811470f3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 0.00001 # add a super small number to ensure we won't run into a division by 0\n",
    "    \n",
    "def Dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(K.cast(y_true, 'float32'))  # Ensuring the type is float32\n",
    "    y_pred_f = K.flatten(K.cast(y_pred, 'float32'))\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth) # val between 0 & 1, 1 means perfect agreement between predicted and true masks\n",
    "\n",
    "def Dice_loss(y_true, y_pred):\n",
    "    return 1.-Dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "def BCE_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + Dice_loss(y_true, y_pred)\n",
    "\n",
    "\n",
    "def Jaccard_coef(y_true, y_predicted): # true and predicted binary mask (function that maps bit image into 0 and 1 pixel values)\n",
    "    intersection = K.sum(y_true*y_predicted, axis=[2, -1, -2]) # K.sum for element wise multiplication, over what axis will the intersection be mapped\n",
    "    sum_union = K.sum(y_true+y_predicted, axis=[2, -1, -2]) # union so addition\n",
    "    JC = (intersection+smooth)/(sum_union-intersection+smooth) # formula for Jaccard coeff, but added smooth to prevent division by a zero!\n",
    "    return K.mean(JC) # return mean of Jaccard coefs (mean for the whole image dataset, 50 pics? 50 JC coeffs -> take mean to get 1)\n",
    "\n",
    "def Jaccard_loss(y_true, y_predicted):\n",
    "    return 1-Jaccard_coef(y_true, y_predicted) # make sure not JC!! bc that's inside the other func\n",
    "        \n",
    "\n",
    "def Focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred)) # pt is probability\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6698acc-52b0-4a7c-a53f-fb27270c64ec",
   "metadata": {},
   "source": [
    "UNET Model Architecture\n",
    "\n",
    "- defined the model's structure, how it should compile, including what optimizer, loss function, and metrics to use\n",
    "- choose one loss function at a time and comment out the others PER RUN\n",
    "- change if needs improvement: activation functions\n",
    "- change if needs improvement: sigmoid for binary, check if should change to multiclass (softmax) \n",
    "- change if needs improvement: Adam optimizer with a learning rate of 1e-2 open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd079fa0-6cf9-44a1-9a8b-3f0f613ded71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(input_size = (512,512,1)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9) \n",
    "\n",
    "    model = Model(inputs = inputs, outputs = conv10)\n",
    "\n",
    "    model.summary()\n",
    "    return model # should return UNcompiled model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7a0317-84fa-4ad5-85e6-00284d387a8e",
   "metadata": {},
   "source": [
    "Standard Training\n",
    "- save the model with the best validation (development) accuracy until now\n",
    "- a checkpoint saves the model's current state, including its weights, so we can resume training later/ use model from a particular epoch\n",
    "- have different checks for different loss functions/metrics\n",
    "- might need to use legacy version of Adam optimizer located at tf.keras.optimizers.legacy.Adam if too slow training times\n",
    "\n",
    "Profiling Training\n",
    "- WRITE IN command line: tensorboard --logdir logs/fit\n",
    "- creates web page to view metrics\n",
    "- Performance profiling includes measuring time the model spends on CPU/GPU/how the time is distributed across different operations\n",
    "- useful to diagnose bottlenecks in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1b0cf05-9217-4f72-adc3-f3b35fb886fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)        [(None, 512, 512, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_168 (Conv2D)         (None, 512, 512, 64)         640       ['input_8[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_169 (Conv2D)         (None, 512, 512, 64)         36928     ['conv2d_168[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_28 (MaxPooli  (None, 256, 256, 64)         0         ['conv2d_169[0][0]']          \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_170 (Conv2D)         (None, 256, 256, 128)        73856     ['max_pooling2d_28[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_171 (Conv2D)         (None, 256, 256, 128)        147584    ['conv2d_170[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_29 (MaxPooli  (None, 128, 128, 128)        0         ['conv2d_171[0][0]']          \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_172 (Conv2D)         (None, 128, 128, 256)        295168    ['max_pooling2d_29[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_173 (Conv2D)         (None, 128, 128, 256)        590080    ['conv2d_172[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_30 (MaxPooli  (None, 64, 64, 256)          0         ['conv2d_173[0][0]']          \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_174 (Conv2D)         (None, 64, 64, 512)          1180160   ['max_pooling2d_30[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_175 (Conv2D)         (None, 64, 64, 512)          2359808   ['conv2d_174[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)        (None, 64, 64, 512)          0         ['conv2d_175[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_31 (MaxPooli  (None, 32, 32, 512)          0         ['dropout_14[0][0]']          \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_176 (Conv2D)         (None, 32, 32, 1024)         4719616   ['max_pooling2d_31[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_177 (Conv2D)         (None, 32, 32, 1024)         9438208   ['conv2d_176[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)        (None, 32, 32, 1024)         0         ['conv2d_177[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_28 (UpSampli  (None, 64, 64, 1024)         0         ['dropout_15[0][0]']          \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_178 (Conv2D)         (None, 64, 64, 512)          2097664   ['up_sampling2d_28[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate_28 (Concatenat  (None, 64, 64, 1024)         0         ['dropout_14[0][0]',          \n",
      " e)                                                                  'conv2d_178[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_179 (Conv2D)         (None, 64, 64, 512)          4719104   ['concatenate_28[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_180 (Conv2D)         (None, 64, 64, 512)          2359808   ['conv2d_179[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_29 (UpSampli  (None, 128, 128, 512)        0         ['conv2d_180[0][0]']          \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_181 (Conv2D)         (None, 128, 128, 256)        524544    ['up_sampling2d_29[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate_29 (Concatenat  (None, 128, 128, 512)        0         ['conv2d_173[0][0]',          \n",
      " e)                                                                  'conv2d_181[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_182 (Conv2D)         (None, 128, 128, 256)        1179904   ['concatenate_29[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_183 (Conv2D)         (None, 128, 128, 256)        590080    ['conv2d_182[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_30 (UpSampli  (None, 256, 256, 256)        0         ['conv2d_183[0][0]']          \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_184 (Conv2D)         (None, 256, 256, 128)        131200    ['up_sampling2d_30[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate_30 (Concatenat  (None, 256, 256, 256)        0         ['conv2d_171[0][0]',          \n",
      " e)                                                                  'conv2d_184[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_185 (Conv2D)         (None, 256, 256, 128)        295040    ['concatenate_30[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_186 (Conv2D)         (None, 256, 256, 128)        147584    ['conv2d_185[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_31 (UpSampli  (None, 512, 512, 128)        0         ['conv2d_186[0][0]']          \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_187 (Conv2D)         (None, 512, 512, 64)         32832     ['up_sampling2d_31[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate_31 (Concatenat  (None, 512, 512, 128)        0         ['conv2d_169[0][0]',          \n",
      " e)                                                                  'conv2d_187[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_188 (Conv2D)         (None, 512, 512, 64)         73792     ['concatenate_31[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_189 (Conv2D)         (None, 512, 512, 64)         36928     ['conv2d_188[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_190 (Conv2D)         (None, 512, 512, 2)          1154      ['conv2d_189[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_191 (Conv2D)         (None, 512, 512, 1)          3         ['conv2d_190[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 31031685 (118.38 MB)\n",
      "Trainable params: 31031685 (118.38 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)        [(None, 512, 512, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_168 (Conv2D)         (None, 512, 512, 64)         640       ['input_8[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_169 (Conv2D)         (None, 512, 512, 64)         36928     ['conv2d_168[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_28 (MaxPooli  (None, 256, 256, 64)         0         ['conv2d_169[0][0]']          \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_170 (Conv2D)         (None, 256, 256, 128)        73856     ['max_pooling2d_28[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_171 (Conv2D)         (None, 256, 256, 128)        147584    ['conv2d_170[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_29 (MaxPooli  (None, 128, 128, 128)        0         ['conv2d_171[0][0]']          \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_172 (Conv2D)         (None, 128, 128, 256)        295168    ['max_pooling2d_29[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_173 (Conv2D)         (None, 128, 128, 256)        590080    ['conv2d_172[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_30 (MaxPooli  (None, 64, 64, 256)          0         ['conv2d_173[0][0]']          \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_174 (Conv2D)         (None, 64, 64, 512)          1180160   ['max_pooling2d_30[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_175 (Conv2D)         (None, 64, 64, 512)          2359808   ['conv2d_174[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)        (None, 64, 64, 512)          0         ['conv2d_175[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_31 (MaxPooli  (None, 32, 32, 512)          0         ['dropout_14[0][0]']          \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_176 (Conv2D)         (None, 32, 32, 1024)         4719616   ['max_pooling2d_31[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_177 (Conv2D)         (None, 32, 32, 1024)         9438208   ['conv2d_176[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)        (None, 32, 32, 1024)         0         ['conv2d_177[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_28 (UpSampli  (None, 64, 64, 1024)         0         ['dropout_15[0][0]']          \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_178 (Conv2D)         (None, 64, 64, 512)          2097664   ['up_sampling2d_28[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate_28 (Concatenat  (None, 64, 64, 1024)         0         ['dropout_14[0][0]',          \n",
      " e)                                                                  'conv2d_178[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_179 (Conv2D)         (None, 64, 64, 512)          4719104   ['concatenate_28[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_180 (Conv2D)         (None, 64, 64, 512)          2359808   ['conv2d_179[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_29 (UpSampli  (None, 128, 128, 512)        0         ['conv2d_180[0][0]']          \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_181 (Conv2D)         (None, 128, 128, 256)        524544    ['up_sampling2d_29[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate_29 (Concatenat  (None, 128, 128, 512)        0         ['conv2d_173[0][0]',          \n",
      " e)                                                                  'conv2d_181[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_182 (Conv2D)         (None, 128, 128, 256)        1179904   ['concatenate_29[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_183 (Conv2D)         (None, 128, 128, 256)        590080    ['conv2d_182[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_30 (UpSampli  (None, 256, 256, 256)        0         ['conv2d_183[0][0]']          \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_184 (Conv2D)         (None, 256, 256, 128)        131200    ['up_sampling2d_30[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate_30 (Concatenat  (None, 256, 256, 256)        0         ['conv2d_171[0][0]',          \n",
      " e)                                                                  'conv2d_184[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_185 (Conv2D)         (None, 256, 256, 128)        295040    ['concatenate_30[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_186 (Conv2D)         (None, 256, 256, 128)        147584    ['conv2d_185[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling2d_31 (UpSampli  (None, 512, 512, 128)        0         ['conv2d_186[0][0]']          \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_187 (Conv2D)         (None, 512, 512, 64)         32832     ['up_sampling2d_31[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate_31 (Concatenat  (None, 512, 512, 128)        0         ['conv2d_169[0][0]',          \n",
      " e)                                                                  'conv2d_187[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_188 (Conv2D)         (None, 512, 512, 64)         73792     ['concatenate_31[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_189 (Conv2D)         (None, 512, 512, 64)         36928     ['conv2d_188[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_190 (Conv2D)         (None, 512, 512, 2)          1154      ['conv2d_189[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_191 (Conv2D)         (None, 512, 512, 1)          3         ['conv2d_190[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 31031685 (118.38 MB)\n",
      "Trainable params: 31031685 (118.38 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Checkpoint will save the model at: models/cnn-parameters-improvement-{epochs}-{Dice_loss:.2f}.model, monitoring 'Dice_loss' for improvements.\n",
      "Epoch 1/30\n",
      "  54/1226 [>.............................] - ETA: 2:18:22 - loss: 0.9983 - Dice_coef: 0.0017"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 22\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint will save the model at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, monitoring \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for improvements.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 4. Set up TensorBoard callback for profiling\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") # logs to be written here, appends tcurrent date/time\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch='10,20') # freq 1=histograms logged after every epoch; profiles 10th to 20th batch of every epoch\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 5. Train model \u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#verbose 1 or 2 to see outputs better\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#callbacks=[checkpoint, tensorboard_callback] # add tensorboard_callback here for profiling\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# 6. Save model\u001b[39;00m\n\u001b[1;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_trained_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/SPR24ENV/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/SPR24ENV/lib/python3.8/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/SPR24ENV/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/SPR24ENV/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/SPR24ENV/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/SPR24ENV/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/SPR24ENV/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/SPR24ENV/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/miniconda3/envs/SPR24ENV/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/SPR24ENV/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. Initialize model\n",
    "model = unet() \n",
    "print(model.summary()) # how many parameters? is architecture correct?\n",
    "\n",
    "# 2. Compile model with selected metric/loss\n",
    "model.compile(optimizer=Adam(learning_rate=1e-2), loss=Dice_loss, metrics=[Dice_coef]) # Dice\n",
    "# model.compile(optimizer = Adam(learning_rate = 1e-2), loss=BCE_loss, metrics=[Dice_coef]) # BCE\n",
    "# model.compile(optimizer=Adam(learning_rate=1e-2), loss=Jaccard_loss, metrics=[Jaccard_coef]) # Jaccard\n",
    "# model.compile(optimizer=Adam(learning_rate=1e-2), loss=Focal_loss, metrics=[Dice_coef]) # Focal\n",
    "\n",
    "# 3. Define checkpoint\n",
    "metric_name = 'Dice_loss'  # Change this accordingly\n",
    "filepath = f\"models/cnn-parameters-improvement-{{epochs}}-{{{metric_name}:.2f}}.model\" \n",
    "checkpoint = ModelCheckpoint(filepath, monitor=metric_name, verbose=1, save_best_only=True, mode='min') # set as 'min' for losses, 'max' for coefs\n",
    "print(f\"Checkpoint will save the model at: {filepath}, monitoring '{metric_name}' for improvements.\")\n",
    "\n",
    "# 4. Set up TensorBoard callback for profiling\n",
    "#log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") # logs to be written here, appends tcurrent date/time\n",
    "#tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch='10,20') # freq 1=histograms logged after every epoch; profiles 10th to 20th batch of every epoch\n",
    "\n",
    "# 5. Train model \n",
    "model.fit(\n",
    "    images_train, masks_train,\n",
    "    batch_size=2,\n",
    "    epochs=30,\n",
    "    verbose=1, #verbose 1 or 2 to see outputs better\n",
    "    validation_data=(images_val, masks_val),\n",
    "    callbacks=[checkpoint] \n",
    "    #callbacks=[checkpoint, tensorboard_callback] # add tensorboard_callback here for profiling\n",
    ")\n",
    "\n",
    "# 6. Save model\n",
    "model.save('first_trained_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d86182-700f-4a81-bb7f-1a35a2be7857",
   "metadata": {},
   "source": [
    "Optional Training with Data augmentation\n",
    "- for generalizability, to potentially help the accuracy score\n",
    "- if using this data augmentation code block, comment OUT unaugmented training block\n",
    "\n",
    "Dimension issue:\n",
    "images_train.shape[0]: original batch size --> Batch size: number of images processed in a single batch\n",
    "images_train.shape[1]: original height --> Height: vertical size of the images\n",
    "images_train.shape[2]: original width --> Width: horizontal size of the images\n",
    "1: Adding single channel dimension for grayscale --> Channels: depth of the image, 1 for grayscale, 3 for RGB color images\n",
    "\n",
    "Batch size: The number of images processed in a single batch.\n",
    "Height: The vertical size of the images.\n",
    "Width: The horizontal size of the images.\n",
    "Channels: The depth of the image, e.g., 1 for grayscale images, 3 for RGB color images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15c8766-d767-4546-bd21-c509b3f32bc7",
   "metadata": {},
   "source": [
    "# was missing channels dimension (rank 4), adding it back as 1\n",
    "if images_train.ndim == 3: # check if array is 3-dimensional\n",
    "    images_train = images_train.reshape(images_train.shape[0], images_train.shape[1], images_train.shape[2], 1) # reshape array +1 dimension\n",
    "if masks_train.ndim == 3:\n",
    "    masks_train = masks_train.reshape(masks_train.shape[0], masks_train.shape[1], masks_train.shape[2], 1)\n",
    "if images_val.ndim == 3:\n",
    "    images_val = images_val.reshape(images_val.shape[0], images_val.shape[1], images_val.shape[2], 1)\n",
    "if masks_val.ndim == 3:\n",
    "    masks_val = masks_val.reshape(masks_val.shape[0], masks_val.shape[1], masks_val.shape[2], 1)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# 1. Initialize model\n",
    "model = unet() \n",
    "print(model.summary()) \n",
    "\n",
    "# 2. Compile model with selected metric/loss\n",
    "model.compile(optimizer=Adam(learning_rate=1e-2), loss=Dice_loss, metrics=[Dice_coef]) # Dice\n",
    "# model.compile(optimizer = Adam(learning_rate = 1e-2), loss=BCE_loss, metrics=[Dice_coef]) # BCE\n",
    "# model.compile(optimizer=Adam(learning_rate=1e-2), loss=Jaccard_loss, metrics=[Jaccard_coef]) # Jaccard\n",
    "# model.compile(optimizer=Adam(learning_rate=1e-2), loss=Focal_loss, metrics=[Dice_coef]) # Focal\n",
    "\n",
    "# 3. Define checkpoint\n",
    "filepath = \"models/cnn-parameters-improvement-{epoch:02d}-{Dice_coef:.2f}.model\"  # Adjust 'dice_coef' if monitoring a different metric\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='Dice_coef', verbose=1, save_best_only=True, mode='max')\n",
    "print(f\"Checkpoint will save data augmented model at: {filepath}, monitoring '{checkpoint.monitor}' for improvements.\")\n",
    "\n",
    "# 4. Train model using training data generator\n",
    "model.fit(datagen.flow(images_train, masks_train, batch_size=2), epochs=30, verbose=1, validation_data=(images_val, masks_val), callbacks=[checkpoint])\n",
    "\n",
    "# 5. Save model\n",
    "model.save('first_data_augmented_trained_model.h5')  # Using '.h5' extension to indicate the model file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1416996-66d4-4695-a024-f51191a08dda",
   "metadata": {},
   "source": [
    "Model Evaluation\n",
    "- evaluate the model with the test set to see how well it performs on unseen data\n",
    "- DONT need to change the evaluation code (model.evaluate) for different metrics; it dynamically adapts based on how the model was compiled\n",
    "- we want our coefficent high and our loss low (only high coefficients should get through and get higher accuracy)\n",
    "- check how well the different loss functions/metrics did\n",
    "- test different batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7212511c-fab2-4376-b5e5-311eaac9c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "results = model.evaluate(images_test, masks_test, batch_size=2)\n",
    "print(f'Test results - Loss: {results[0]}, Metric (as per chosen during compilation): {results[1]}') # should return (1st) test loss value (2nd) metrics value(s) as list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6236416e-1fe4-43aa-b78b-59781f4c4dc8",
   "metadata": {},
   "source": [
    "Performance Interpretation (UNcomment the selected metric ahead of time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a983047e-7c62-4086-af2c-c6d127da774c",
   "metadata": {},
   "source": [
    "dice_score = results[1]\n",
    "print(f\"Dice Coefficient: {dice_score:.3f}\")\n",
    "if dice_score > 0.7:\n",
    "    print(\"Excellent segmentation performance.\")\n",
    "elif dice_score > 0.5:\n",
    "    print(\"Good segmentation performance.\")\n",
    "else:\n",
    "    print(\"Poor segmentation performance, move to further training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe085a9f-7b18-4383-9f91-8ca2447d8d9d",
   "metadata": {},
   "source": [
    "Optional Further Training\n",
    "- if believed the model can improve with more epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f56993-cc63-45b5-a27c-2d23415e9db9",
   "metadata": {},
   "source": [
    "model.fit(images_train, masks_train, batch_size=2, epochs=3, verbose=1, validation_data=(images_val, masks_val), callbacks=[checkpoint])\n",
    "model.save('second_trained_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e1165b-0a1c-4178-8843-8c07f41c87f0",
   "metadata": {},
   "source": [
    "Final Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b771d314-b51c-496f-b18f-b1857b314fad",
   "metadata": {},
   "source": [
    "results = model.evaluate(images_test, masks_test, batch_size=2)\n",
    "print(f'Final test results - Loss: {results[0]}, Metric (as per chosen during compilation): {results[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (UNETENV)",
   "language": "python",
   "name": "unetenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
