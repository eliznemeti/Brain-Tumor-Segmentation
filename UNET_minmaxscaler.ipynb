{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b788f07-20f4-4ea6-bc73-30ad08ce6adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2 #opencv+python == 4.9.0.80\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c8d0a3-f232-4260-a705-25e2e5a60604",
   "metadata": {},
   "source": [
    "Data Loading/Preprocessing/Image Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e561ca42-ca47-46f6-b093-9f2a597033e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to normalize pixel values\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# looping through files to call the scaling/ alt way for this line: image = scaler.fit_transform(image.reshape(-1, 1)).reshape(image.shape)\n",
    "# for i in range(images.shape[0]):\n",
    "#    images[i, :, :] = scaler.fit_transform(images[i, :, :].reshape(-1, 1)).reshape(images[i, :, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0e4eb2-28a2-45b3-96a3-490b8b76f33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping both i and file (2 vars)\n",
    "# start at 1 bc data files starts at 1\n",
    "for i, file in enumerate(files, start=1):\n",
    "    if i % 10 == 0: # we can check the loading progress by 10s as % (like a loading bar for visualizing)\n",
    "        sys.stdout.write('\\r[{}/{}] images loaded: {:.1f} %'.format(i, len(files), i / float(len(files)) * 100))\n",
    "        sys.stdout.flush() # \"flush\" good to add, it makes progress immediately visible instead of waiting for compiling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bb8cce-63ab-48f5-a01d-f0a54df6e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before minmaxing we want to resize the images (w Cv2)\n",
    "# 1. resize 2. store for later use in script\n",
    "    mat_file = hdf5storage.loadmat(os.path.join(data_dir, file))['cjdata'][0]\n",
    "    image = cv2.resize(mat_file[2], dsize=(args.image_dimension, args.image_dimension), # default is usually 512, but args.image_dimension is a command line tool that will select the dimension for us (resize itself)\n",
    "                       interpolation=cv2.INTER_CUBIC)\n",
    "    mask = cv2.resize(mat_file[4].astype('uint8'), dsize=(args.image_dimension, args.image_dimension), # same for mask as for image\n",
    "                      interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "image = scaler.fit_transform(image.reshape(-1, 1)).reshape(image.shape) # here we apply the scalar, and we transposed the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900d664-70a9-4691-a942-7dc9a2f91c58",
   "metadata": {},
   "source": [
    "Histogram Equalizer (method 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93909a4-8f2b-43f8-8cc6-7da8b9d17a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.equalizeHist((image * 255).astype(np.uint8))/255.0 \n",
    "# each pixel value gets x by 255\n",
    "# why uint8? to cast as a type that is unassigned (no +/-), (common when working with image intensity)\n",
    "# need the .0 bc equalizer spits out float, so to help it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2693a89a-9f01-4ee3-af4d-f1314d1c0a14",
   "metadata": {},
   "source": [
    "Contrast Adjustment (or method 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c5288-3f11-4a8f-8de7-c645e80451b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 1.5 # remember alpha is multiplied by source\n",
    "# beta = 10 # remember beta is added to source\n",
    "# image = cv2.convertScaleAbs(image, alpha, beta) # src = image, alpha = contrast, beta = brightness, convert = saturate the image\n",
    "# # we're taking in the source, alpha, beta and multiplying the \n",
    "# # increase brightness b > 0, decrease b < 0\n",
    "# # increase contrast a > 1 (cant multiply by 0!), or decrease a < 1, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcedfeb-3b6f-4cce-9b7c-a9001b36dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.append(int(mat_file[0])) # fixing our labels because of the transpose \n",
    "images.append(image) # appending the resized images to the variable of the initial images\n",
    "masks.append(mask.astype(bool)) # aka booleans only, is there a mask or not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2871b23f-79e1-4fd6-b334-f01688b8a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels) # converting labels to numpy arrays\n",
    "images = np.array(images) #\"\"\n",
    "masks = np.array(masks) #\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b584bad-fe4e-485c-ba97-81b677783136",
   "metadata": {},
   "source": [
    "Apply data normalizaiton to the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ce488-aafb-434b-bffa-6f79ec451eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## following chunk of code is run after splitting data into both training and testing ##\n",
    "images_train = scaler.fit_transform(images_train.reshape(-1, 1)).reshape(images_train.shape)\n",
    "images_cv = scaler.fit_transform(images_cv.reshape(-1, 1)).reshape(images_cv.shape)\n",
    "images_test = scaler.fit_transform(images_test.reshape(-1, 1)).reshape(images_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c46906-1e8a-447f-9630-92b7e8ed2afb",
   "metadata": {},
   "source": [
    "Data augmentation (for generalizability, to potentially help the accuracy score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0151f7c4-b2e0-4f0e-ac8b-15a685bfc05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2, # equal force from opposite directions\n",
    "    zoom_range=0.2, # zooming through the image\n",
    "    horizontal_flip=True, # mirror it\n",
    "    fill_mode='nearest' \n",
    ")\n",
    "\n",
    "# train the model using training data generated and augmented parameters\n",
    "model.fit(datagen.flow(images_train, masks_train, batch_size=2), epochs=30, verbose=1, validation_data=(images_cv, masks_cv), callbacks=[checkpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SPR24ENV)",
   "language": "python",
   "name": "spr24env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
